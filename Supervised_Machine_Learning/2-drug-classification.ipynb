{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1419436,"sourceType":"datasetVersion","datasetId":830916}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Drug Classification(EDA & ML Classification)","metadata":{}},{"cell_type":"markdown","source":"## 1. Dataset Description\nThere are 6 variables in the dataset:\n- Age\n- Sex\n- BP(Blood Pressure Levels)\n- Cholesterol\n- Sodium to Potassium Ratio\n- Drug Type(Target)","metadata":{}},{"cell_type":"markdown","source":"## 2. Imports and Dataset Exploration","metadata":{}},{"cell_type":"code","source":"# library imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set_theme(style = 'darkgrid')","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:31.348119Z","iopub.execute_input":"2023-12-10T20:36:31.348669Z","iopub.status.idle":"2023-12-10T20:36:31.362440Z","shell.execute_reply.started":"2023-12-10T20:36:31.348593Z","shell.execute_reply":"2023-12-10T20:36:31.360457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:31.366130Z","iopub.execute_input":"2023-12-10T20:36:31.366640Z","iopub.status.idle":"2023-12-10T20:36:31.381229Z","shell.execute_reply.started":"2023-12-10T20:36:31.366599Z","shell.execute_reply":"2023-12-10T20:36:31.379027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df import\ndf = pd.read_csv(r'/kaggle/input/drug-classification/drug200.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:31.384105Z","iopub.execute_input":"2023-12-10T20:36:31.384761Z","iopub.status.idle":"2023-12-10T20:36:31.409930Z","shell.execute_reply.started":"2023-12-10T20:36:31.384710Z","shell.execute_reply":"2023-12-10T20:36:31.408354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:31.412293Z","iopub.execute_input":"2023-12-10T20:36:31.413184Z","iopub.status.idle":"2023-12-10T20:36:31.423227Z","shell.execute_reply.started":"2023-12-10T20:36:31.413143Z","shell.execute_reply":"2023-12-10T20:36:31.422181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As it can be seen; the dataset has no empty cells, therefore it does not require any operations the drop empty columns or cleaning.\nA function called 'Plotter' will be created to plot each variables' density plot. It will take the dataframe as an input; create a list out of its column names; then plot the density plots ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def density_plotter(df):\n    variable_names = list(df.columns)\n    number_of_plots = len(variable_names)\n    number_of_rows = (number_of_plots + 2) // 3\n    fig, axs = plt.subplots(number_of_rows, 3, figsize = (20, 10))\n    \n    # Plotting\n    for i, variable_name in enumerate(variable_names):\n        row = i // 3 #Can be only 0 and 1 since index starts from 0 and ends at 5\n        col = i % 3 # Can only be [0, 1, 2].\n        axs[row, col].hist(df[variable_name], bins = 20, density = True, alpha = 0.5)\n        axs[row, col].set_xlabel(variable_name)\n        axs[row, col].set_ylabel('Density')\n    plt.tight_layout()\n    plt.show()\n\ndensity_plotter(df)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:31.426174Z","iopub.execute_input":"2023-12-10T20:36:31.427530Z","iopub.status.idle":"2023-12-10T20:36:33.385495Z","shell.execute_reply.started":"2023-12-10T20:36:31.427484Z","shell.execute_reply":"2023-12-10T20:36:33.384438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- \"Age\" variable is symetrically distributed.\n- \"Gender\" variable is evenly distributed between Male and Female.\n- \"Cholesterol\" variable is evenly distributed between High and Low.\n- \"Na_to_K\" variable is slightly skewed.\n","metadata":{}},{"cell_type":"code","source":"def count_plotter(df):\n    variable_names = list(df.columns)\n    number_of_plots = len(variable_names)\n    number_of_rows = (number_of_plots + 2) // 3\n    fig, axs = plt.subplots(number_of_rows, 3, figsize = (20, 10))\n    \n    # Plotting\n    for i, variable_name in enumerate(variable_names):\n        row = i // 3 #Can be only 0 and 1 since index starts from 0 and ends at 5\n        col = i % 3 # Can only be [0, 1, 2].\n        axs[row, col].hist(df[variable_name], bins = 20, alpha = 0.5)\n        axs[row, col].set_xlabel(variable_name)\n        axs[row, col].set_ylabel('Count')\n    plt.tight_layout()\n    plt.show()\n\ncount_plotter(df)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:33.386877Z","iopub.execute_input":"2023-12-10T20:36:33.387468Z","iopub.status.idle":"2023-12-10T20:36:35.362627Z","shell.execute_reply.started":"2023-12-10T20:36:33.387432Z","shell.execute_reply":"2023-12-10T20:36:35.361027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"Age\" and \"Na_to_K\" variables have to be grouped for better data analysis.\nAge will be grouped as:\n- Under 25\n- 25-35\n- 35-45\n- 45-55\n- 55-65\n- Over 65\n\nNa_to_K will be grouped as:\n- Under 5\n- 5-15\n- 15-25\n- 25-35\n- Over 35","metadata":{}},{"cell_type":"code","source":"age_grouped = []\nna_to_k_grouped = []\nfor i in df['Na_to_K']:\n    if i <= 5:\n        na_to_k_grouped.append('0-5')\n    elif i > 5 and i <= 15:\n        na_to_k_grouped.append('5-15')\n    elif i > 15 and i <= 25:\n        na_to_k_grouped.append('15-25')\n    elif i > 25 and i <= 35:\n        na_to_k_grouped.append('25-35')\n    else:\n        na_to_k_grouped.append('35+')\n        \nfor i in df['Age']:\n    if i <= 25:\n        age_grouped.append('0-25')\n    elif i > 25 and i <= 35:\n        age_grouped.append('25-35')\n    elif i > 35 and i <= 45:\n        age_grouped.append('35-45')\n    elif i > 45 and i <= 55:\n        age_grouped.append('45-55')\n    elif i > 55 and i <= 65:\n        age_grouped.append('55-65')\n    else:\n        age_grouped.append('65+')\n        \ndf['Age_Grouped'] = age_grouped\ndf[\"Na_to_K_Grouped\"] = na_to_k_grouped\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:35.365049Z","iopub.execute_input":"2023-12-10T20:36:35.365978Z","iopub.status.idle":"2023-12-10T20:36:35.395683Z","shell.execute_reply.started":"2023-12-10T20:36:35.365898Z","shell.execute_reply":"2023-12-10T20:36:35.393804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = 'Age_Grouped', data = df, palette = 'pastel')","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:35.398286Z","iopub.execute_input":"2023-12-10T20:36:35.398822Z","iopub.status.idle":"2023-12-10T20:36:35.786409Z","shell.execute_reply.started":"2023-12-10T20:36:35.398775Z","shell.execute_reply":"2023-12-10T20:36:35.785165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = 'Na_to_K_Grouped', data = df, palette = 'pastel')","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:35.788212Z","iopub.execute_input":"2023-12-10T20:36:35.788684Z","iopub.status.idle":"2023-12-10T20:36:36.121192Z","shell.execute_reply.started":"2023-12-10T20:36:35.788605Z","shell.execute_reply":"2023-12-10T20:36:36.119835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = df['Drug'], hue = \"Sex\", data = df)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:36.122897Z","iopub.execute_input":"2023-12-10T20:36:36.123360Z","iopub.status.idle":"2023-12-10T20:36:36.529603Z","shell.execute_reply.started":"2023-12-10T20:36:36.123325Z","shell.execute_reply":"2023-12-10T20:36:36.528097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = df['Drug'], hue = \"BP\", data = df)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:36.535407Z","iopub.execute_input":"2023-12-10T20:36:36.536730Z","iopub.status.idle":"2023-12-10T20:36:36.983898Z","shell.execute_reply.started":"2023-12-10T20:36:36.536674Z","shell.execute_reply":"2023-12-10T20:36:36.982458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = df['Drug'], hue = \"Cholesterol\", data = df)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:36.985668Z","iopub.execute_input":"2023-12-10T20:36:36.986164Z","iopub.status.idle":"2023-12-10T20:36:37.423455Z","shell.execute_reply.started":"2023-12-10T20:36:36.986118Z","shell.execute_reply":"2023-12-10T20:36:37.421712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(y = 'Drug', data = df, palette = 'pastel')\nplt.title('Number of Each Drug Type')\nplt.xlabel('Count')\nplt.ylabel(\"Drug Type\")","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:37.425745Z","iopub.execute_input":"2023-12-10T20:36:37.426309Z","iopub.status.idle":"2023-12-10T20:36:37.866701Z","shell.execute_reply.started":"2023-12-10T20:36:37.426259Z","shell.execute_reply":"2023-12-10T20:36:37.865243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As it can be seen from the figure above, the drug type \"DrugY\" dominates the dataset. To avoid overfitting; an oversampling technique called SMOTE(Synthetic Minority Oversampling Technique) will be used to oversample other drug types.","metadata":{}},{"cell_type":"code","source":"# Some insights from the dataset\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:37.868657Z","iopub.execute_input":"2023-12-10T20:36:37.869874Z","iopub.status.idle":"2023-12-10T20:36:37.887395Z","shell.execute_reply.started":"2023-12-10T20:36:37.869822Z","shell.execute_reply":"2023-12-10T20:36:37.885711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As it can be seen from the info snippet above; there are multiple variables with object data type. The objects will be processed with \"labelencoder\" from the \"sklearn.preprocessing\" library.","metadata":{}},{"cell_type":"markdown","source":"## 3. Data Preprocessing and SMOTE(Synthetic Minority Oversampling Technique)","metadata":{}},{"cell_type":"markdown","source":"The grouped data is not needed for the machine learning algorithms, they were only implemented to show the data distribution. Therefore two columns called \"Age_Grouped\" and \"Na_to_K_Grouped\" will be dropped.","metadata":{}},{"cell_type":"code","source":"df = df.drop([\"Age_Grouped\", \"Na_to_K_Grouped\"], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:37.889348Z","iopub.execute_input":"2023-12-10T20:36:37.889814Z","iopub.status.idle":"2023-12-10T20:36:37.900275Z","shell.execute_reply.started":"2023-12-10T20:36:37.889778Z","shell.execute_reply":"2023-12-10T20:36:37.898973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:37.902794Z","iopub.execute_input":"2023-12-10T20:36:37.903850Z","iopub.status.idle":"2023-12-10T20:36:37.924439Z","shell.execute_reply.started":"2023-12-10T20:36:37.903796Z","shell.execute_reply":"2023-12-10T20:36:37.922500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:37.926521Z","iopub.execute_input":"2023-12-10T20:36:37.927340Z","iopub.status.idle":"2023-12-10T20:36:37.935187Z","shell.execute_reply.started":"2023-12-10T20:36:37.927299Z","shell.execute_reply":"2023-12-10T20:36:37.933402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in list(df.columns):\n    if df[i].dtype == 'object':\n        df[i] = LabelEncoder().fit_transform(df[i])","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:37.937866Z","iopub.execute_input":"2023-12-10T20:36:37.938332Z","iopub.status.idle":"2023-12-10T20:36:37.950269Z","shell.execute_reply.started":"2023-12-10T20:36:37.938297Z","shell.execute_reply":"2023-12-10T20:36:37.948990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:37.951897Z","iopub.execute_input":"2023-12-10T20:36:37.952489Z","iopub.status.idle":"2023-12-10T20:36:37.977231Z","shell.execute_reply.started":"2023-12-10T20:36:37.952438Z","shell.execute_reply":"2023-12-10T20:36:37.975175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now all variables have the datatype of 'int64', which makes every variable usable in the classification algorithms to be implemented.","metadata":{}},{"cell_type":"code","source":"correlation_matrix = df.corr()\n\n# Set the figure size\nplt.figure(figsize=(7, 4))\n\n# Create a heatmap\nheatmap = plt.imshow(correlation_matrix, cmap = 'cubehelix_r', interpolation='nearest')\n\n# Display the correlation values on the heatmap\nfor i in range(len(correlation_matrix)):\n    for j in range(len(correlation_matrix)):\n        plt.text(j, i, f\"{correlation_matrix.iloc[i, j]:.2f}\", ha='center', va='center', color='w')\n\n# Set labels and title\nplt.xticks(range(len(correlation_matrix)), correlation_matrix.columns, rotation=45)\nplt.yticks(range(len(correlation_matrix)), correlation_matrix.columns)\nplt.title(\"Correlation Matrix\")\n\n# Display the colorbar\nplt.colorbar(heatmap)\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:37.979564Z","iopub.execute_input":"2023-12-10T20:36:37.980454Z","iopub.status.idle":"2023-12-10T20:36:40.846972Z","shell.execute_reply.started":"2023-12-10T20:36:37.980392Z","shell.execute_reply":"2023-12-10T20:36:40.845865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"Blood Pressure\" variable has the highest correlation with Drug type.","metadata":{}},{"cell_type":"markdown","source":"## 4. Implementation of ML Algorithms","metadata":{}},{"cell_type":"markdown","source":"First, a function to extract features and target will be created. From previous analysis, the target has been identified as \"Drug\".","metadata":{}},{"cell_type":"code","source":"def train_evaluate(df, features, label, model_list):\n    \"\"\"\n    Train and evaluate machine learning models for a given list of algorithms.\n\n    Parameters:\n    - df (pd.DataFrame): The DataFrame containing the dataset\n    - features (list): List of feature names\n    - label (String): The label\n    - model_list (list): List of machine learning algorithms (pre-instantiated models)\n\n    Returns:\n    - results_df (pd.DataFrame): DataFrame containing algorithm names and corresponding accuracy scores\n    \"\"\"\n    results = []\n    \n    X = df.drop([\"Drug\"], axis = 1)\n    y = df['Drug']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n    \n    for algorithm in model_list:\n        model = algorithm\n        model.fit(X_train, y_train)\n        prediction = model.predict(X_test)\n        accuracy = accuracy_score(prediction, y_test)\n        results.append({'Algorithm': algorithm.__class__.__name__, 'Accuracy %': accuracy * 100})\n\n    results_df = pd.DataFrame(results)\n    return results_df","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:40.848459Z","iopub.execute_input":"2023-12-10T20:36:40.850495Z","iopub.status.idle":"2023-12-10T20:36:40.864165Z","shell.execute_reply.started":"2023-12-10T20:36:40.850426Z","shell.execute_reply":"2023-12-10T20:36:40.862370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = \"Drug\"\nmodel_list = [KNeighborsClassifier(), DecisionTreeClassifier(), RandomForestClassifier(), SVC(), LogisticRegression()]\nfeatures = list(df.columns)\nfeatures.remove(label)\ntrain_evaluate(df, features, label, model_list)","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:40.866297Z","iopub.execute_input":"2023-12-10T20:36:40.869185Z","iopub.status.idle":"2023-12-10T20:36:41.200324Z","shell.execute_reply.started":"2023-12-10T20:36:40.869122Z","shell.execute_reply":"2023-12-10T20:36:41.199005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Hyperparameter Optimization","metadata":{}},{"cell_type":"code","source":"def feature_extractor(df, label):\n    features = list(df.columns)\n    features.remove(label)\n    return features\n","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:41.202135Z","iopub.execute_input":"2023-12-10T20:36:41.202684Z","iopub.status.idle":"2023-12-10T20:36:41.211262Z","shell.execute_reply.started":"2023-12-10T20:36:41.202632Z","shell.execute_reply":"2023-12-10T20:36:41.209391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_params = {\"n_neighbors\": range(2, 50)}\n\ndecision_tree_params = {'max_depth': range(1, 20),\n               \"min_samples_split\": range(2, 30)}\n\nrf_params = {\"max_depth\": [5, 20, None],\n             \"max_features\": [3, 10, \"auto\"],\n             \"min_samples_split\": [10, 20],\n             \"n_estimators\": [150, 300]}\n\nsvc_params = {'C': [0.1, 1, 10, 100],\n              'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n              'gamma': ['scale', 'auto', 0.1, 1, 10],}\n\nlr_params = {'penalty': ['l1', 'l2'],\n             'C': [0.001, 0.01, 0.1, 1, 10, 100],\n             'solver': ['liblinear', 'lbfgs'],}\n\nclassifiers = [(KNeighborsClassifier(), knn_params),\n               (DecisionTreeClassifier(), decision_tree_params),\n               (RandomForestClassifier(), rf_params),\n               (SVC(), svc_params),\n               (LogisticRegression(), lr_params)]","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:41.213747Z","iopub.execute_input":"2023-12-10T20:36:41.214186Z","iopub.status.idle":"2023-12-10T20:36:41.231064Z","shell.execute_reply.started":"2023-12-10T20:36:41.214154Z","shell.execute_reply":"2023-12-10T20:36:41.229625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hyperparameter_opti(df, features, label, model_list):\n    \"\"\"\n    Train and evaluate machine learning models for a given list of algorithms.\n\n    Parameters:\n    - df (pd.DataFrame): The DataFrame containing the dataset\n    - features (list): List of feature names\n    - label (String): The label\n    - model_list (list): List of machine learning algorithms (pre-instantiated models)\n\n    Returns:\n    - results_df (pd.DataFrame): DataFrame containing algorithm names and corresponding accuracy scores\n    \"\"\"\n    results = []\n    \n    X = df[features]\n    y = df[label]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n    \n    for algorithm, params in model_list:\n        # Perform GridSearchCV for hyperparameter tuning\n        gs_best = GridSearchCV(algorithm, params, cv=5, n_jobs=-1, verbose=False).fit(X_train, y_train)\n        final_model = algorithm.set_params(**gs_best.best_params_)\n        \n        # Fit the final model on the entire training set\n        final_model.fit(X_train, y_train)\n        \n        # Evaluate the model using cross-validation\n        cv_scores = cross_val_score(final_model, X_train, y_train, cv=5, scoring='accuracy')\n        mean_cv_accuracy = np.mean(cv_scores)\n        \n        # Make predictions on the test set\n        prediction = final_model.predict(X_test)\n        \n        # Calculate accuracy and add results to the list\n        accuracy = accuracy_score(prediction, y_test)\n        results.append({'Algorithm': algorithm.__class__.__name__, 'Accuracy': accuracy, 'Mean CV Accuracy': mean_cv_accuracy})\n        \n        # Print accuracy score\n        print(f'{algorithm.__class__.__name__} Accuracy score: {accuracy * 100}%')\n        print(f'{algorithm.__class__.__name__} Mean Cross-Validation Accuracy: {mean_cv_accuracy * 100}%\\n')\n    \n    results_df = pd.DataFrame(results)\n    print(results_df)\n    return results_df","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:41.235696Z","iopub.execute_input":"2023-12-10T20:36:41.236304Z","iopub.status.idle":"2023-12-10T20:36:41.249942Z","shell.execute_reply.started":"2023-12-10T20:36:41.236237Z","shell.execute_reply":"2023-12-10T20:36:41.248295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    model_list = [KNeighborsClassifier(), DecisionTreeClassifier(), RandomForestClassifier(), SVC(), LogisticRegression()]\n    label = \"Drug\"\n    features = feature_extractor(df, label)\n    print(f'The features are: {features}')\n    first_results = train_evaluate(df, features, label, model_list)\n    print(f'First results are: {first_results}')\n    print(f\"After Hyperparameter Optimization:\")\n    post_optimization = hyperparameter_opti(df, features, label, classifiers)\n    print(\"Ended.\")\n\nif __name__ == \"__main__\":\n    print(\"End to End\")\n    main()","metadata":{"execution":{"iopub.status.busy":"2023-12-10T20:36:41.252142Z","iopub.execute_input":"2023-12-10T20:36:41.252641Z","iopub.status.idle":"2023-12-10T20:37:29.587496Z","shell.execute_reply.started":"2023-12-10T20:36:41.252590Z","shell.execute_reply":"2023-12-10T20:37:29.584071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Conclusion\nThe evaluation results of various machine learning algorithms on the dataset are presented here. The models were assessed based on both their accuracy on a holdout test set and their mean cross-validation accuracy, providing a more robust estimation of generalization performance.\n\nKNeighborsClassifier:\nAchieved a test set accuracy of 74% and a mean cross-validation accuracy of 70%. This model demonstrated moderate performance on the test set with consistent results across cross-validation folds.\n\nDecisionTreeClassifier:\nAttained a perfect test set accuracy of 100% and a mean cross-validation accuracy of 98.67%. While the DecisionTreeClassifier exhibited perfect accuracy on the test set, suggesting potential overfitting, the high cross-validation accuracy indicates strong generalizability.\n\nRandomForestClassifier:\nAchieved a perfect test set accuracy of 100% and a mean cross-validation accuracy of 98.67%. Similar to the DecisionTreeClassifier, the RandomForestClassifier demonstrated perfect accuracy on the test set and strong performance in cross-validation.\n\nSVC (Support Vector Classifier):\nDemonstrated excellent performance with a test set accuracy of 100% and a mean cross-validation accuracy of 98%. The Support Vector Classifier exhibited high accuracy on the test set and robust performance in cross-validation.\n\nLogisticRegression:\nObtained a test set accuracy of 96% and a mean cross-validation accuracy of 95.33%. The LogisticRegression model achieved high accuracy on the test set and consistent performance in cross-validation, indicating a well-generalized model.\n\nTo enhance the models' performance, hyperparameter optimization was conducted using GridSearchCV. The resulting models showed improved or maintained accuracy levels, highlighting the effectiveness of fine-tuning model parameters.\n\nIn summary, the DecisionTreeClassifier, RandomForestClassifier, SVC, and LogisticRegression models exhibited strong performance on the dataset, with high accuracy and generalization capability. Consideration of other metrics and further investigation into model interpretability can provide additional insights into the models' behavior. The choice of the final model may depend on the specific requirements of the application, such as the importance of interpretability, computational efficiency, or the nature of the data.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}